{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiKv48f/8GA7xpajmQFHtC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCSFsoDFPKAu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- PART 1: SETUP FOR COLAB T4 ---\n",
        "\n",
        "# Define the working directory for Google Colab\n",
        "WORK_DIR = \"/content/ComfyUI\"\n",
        "\n",
        "import os\n",
        "\n",
        "# 1. Clone the repository\n",
        "if not os.path.exists(WORK_DIR):\n",
        "    !git clone https://github.com/Imtiazul-Islam/ComfyUI {WORK_DIR}\n",
        "\n",
        "# Navigate into the directory\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Install requirements (quietly)\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Install aria2 for fast downloads\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "# 2. Download the models using aria2 from your Hugging Face Mirror\n",
        "# Pointing to ImtiazulIslam/Z-Image-Turbo-FP8\n",
        "\n",
        "print(\"Downloading Models (UNET, CLIP, VAE)...\")\n",
        "\n",
        "# Model 1: UNET (Diffusion Model) - FP8 Version\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d {WORK_DIR}/models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n",
        "\n",
        "# Model 2: CLIP (Text Encoder) - Qwen/Lumina type\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/qwen_3_4b.safetensors -d {WORK_DIR}/models/clip -o qwen_3_4b.safetensors\n",
        "\n",
        "# Model 3: VAE\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/ae.safetensors -d {WORK_DIR}/models/vae -o ae.safetensors\n",
        "\n",
        "print(\"Setup Complete. Models loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PART 2: LOAD MODELS & UI (OPTIMIZED FOR COLAB) ---\n",
        "\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Ensure ComfyUI modules are found\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Import ComfyUI Nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "# Initialize Nodes\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "CLIPLoader = NODE_CLASS_MAPPINGS[\"CLIPLoader\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
        "KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "print(\"Loading Models into VRAM (this may take a moment)...\")\n",
        "\n",
        "# Load Models (Using fp8_e4m3fn_fast for T4 efficiency)\n",
        "with torch.inference_mode():\n",
        "    unet = UNETLoader.load_unet(\"z-image-turbo-fp8-e4m3fn.safetensors\", \"fp8_e4m3fn_fast\")[0]\n",
        "    # type=\"lumina2\" is required for this specific CLIP model\n",
        "    clip = CLIPLoader.load_clip(\"qwen_3_4b.safetensors\", type=\"lumina2\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.safetensors\")[0]\n",
        "\n",
        "print(\"Models Loaded Successfully. Initializing UI...\")\n",
        "\n",
        "# ==========================================\n",
        "# GENERATION LOGIC\n",
        "# ==========================================\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_generation(b):\n",
        "    # UI Feedback\n",
        "    gen_btn.disabled = True\n",
        "    gen_btn.description = \"Generating...\"\n",
        "    out_area.clear_output(wait=True)\n",
        "\n",
        "    with out_area:\n",
        "        print(f\"Resolution: {width.value}x{height.value} | Steps: {steps.value} | Seed: {seed.value}\")\n",
        "\n",
        "        tmp_dir = f\"{WORK_DIR}/output\"\n",
        "        os.makedirs(tmp_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Handle Seed\n",
        "        current_seed = seed.value\n",
        "        if current_seed == 0:\n",
        "            random.seed(int(time.time()))\n",
        "            current_seed = random.randint(0, 18446744073709551615)\n",
        "\n",
        "        # 2. Encode Prompts\n",
        "        positive = CLIPTextEncode.encode(clip, positive_prompt.value)[0]\n",
        "        negative = CLIPTextEncode.encode(clip, negative_prompt.value)[0]\n",
        "\n",
        "        # 3. Generate Latent\n",
        "        latent_image = EmptyLatentImage.generate(width.value, height.value, batch_size=1)[0]\n",
        "\n",
        "        # 4. Sample (Diffusion Process)\n",
        "        samples = KSampler.sample(\n",
        "            unet,\n",
        "            current_seed,\n",
        "            steps.value,\n",
        "            cfg.value,\n",
        "            sampler_name.value,\n",
        "            scheduler.value,\n",
        "            positive,\n",
        "            negative,\n",
        "            latent_image,\n",
        "            denoise=denoise.value\n",
        "        )[0]\n",
        "\n",
        "        # 5. Decode VAE\n",
        "        decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
        "\n",
        "        # 6. Save with Timestamp\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"z_turbo_{timestamp}.png\"\n",
        "        output_path = f\"{tmp_dir}/{filename}\"\n",
        "\n",
        "        # Convert tensor to image and save\n",
        "        img_array = np.array(decoded*255, dtype=np.uint8)[0]\n",
        "        Image.fromarray(img_array).save(output_path)\n",
        "\n",
        "        # 7. Display\n",
        "        print(f\"Done! Actual Seed: {current_seed}\")\n",
        "        display(Image.open(output_path))\n",
        "\n",
        "    # Re-enable button\n",
        "    gen_btn.disabled = False\n",
        "    gen_btn.description = \"Generate Image\"\n",
        "\n",
        "def update_dimensions(change):\n",
        "    \"\"\"Auto-update Width/Height based on Aspect Ratio selection\"\"\"\n",
        "    ar = change['new']\n",
        "    if ar == \"Custom\":\n",
        "        return # Don't override if user selects Custom\n",
        "\n",
        "    presets = {\n",
        "        \"1:1 (Square)\": (1024, 1024),\n",
        "        \"16:9 (Landscape)\": (1344, 768), # Slightly adjusted for better mult of 16\n",
        "        \"9:16 (Portrait)\": (768, 1344),\n",
        "        \"4:3 (Photo)\": (1152, 896),\n",
        "        \"3:2 (Classic)\": (1216, 832),\n",
        "        \"21:9 (Ultrawide)\": (1536, 640)\n",
        "    }\n",
        "\n",
        "    if ar in presets:\n",
        "        w, h = presets[ar]\n",
        "        width.value = w\n",
        "        height.value = h\n",
        "\n",
        "# ==========================================\n",
        "# WIDGET DEFINITIONS\n",
        "# ==========================================\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "# Prompts\n",
        "positive_prompt = widgets.Textarea(\n",
        "    value=\"a cinematic photo of a cat, realistic, 8k, highly detailed\",\n",
        "    description='Positive:', style=style, layout=widgets.Layout(width='90%')\n",
        ")\n",
        "negative_prompt = widgets.Textarea(\n",
        "    value=\"blurry, ugly, bad quality, distorted, low res\",\n",
        "    description='Negative:', style=style, layout=widgets.Layout(width='90%')\n",
        ")\n",
        "\n",
        "# Aspect Ratio & Dimensions\n",
        "aspect_ratio = widgets.Dropdown(\n",
        "    options=[\"1:1 (Square)\", \"16:9 (Landscape)\", \"9:16 (Portrait)\", \"4:3 (Photo)\", \"3:2 (Classic)\", \"21:9 (Ultrawide)\", \"Custom\"],\n",
        "    value=\"1:1 (Square)\",\n",
        "    description='Aspect Ratio:', style=style\n",
        ")\n",
        "aspect_ratio.observe(update_dimensions, names='value')\n",
        "\n",
        "width = widgets.IntText(value=1024, description='Width:', style=style)\n",
        "height = widgets.IntText(value=1024, description='Height:', style=style)\n",
        "\n",
        "# Settings\n",
        "steps = widgets.IntSlider(value=6, min=1, max=20, step=1, description='Steps:', style=style) # Z-Turbo works best with low steps (4-6)\n",
        "cfg = widgets.FloatSlider(value=1.2, min=1.0, max=3.0, step=0.1, description='CFG Scale:', style=style)\n",
        "seed = widgets.IntText(value=0, description='Seed (0=Random):', style=style)\n",
        "sampler_name = widgets.Dropdown(\n",
        "    options=['euler', 'euler_ancestral', 'dpmpp_2m', 'dpmpp_sde'],\n",
        "    value='euler', description='Sampler:', style=style\n",
        ")\n",
        "scheduler = widgets.Dropdown(\n",
        "    options=['simple', 'normal', 'karras', 'exponential', 'sgm_uniform'],\n",
        "    value='simple', description='Scheduler:', style=style\n",
        ")\n",
        "denoise = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, description='Denoise:', style=style)\n",
        "\n",
        "# Generate Button\n",
        "gen_btn = widgets.Button(\n",
        "    description='Generate Image',\n",
        "    button_style='success',\n",
        "    tooltip='Click to generate',\n",
        "    icon='magic'\n",
        ")\n",
        "gen_btn.on_click(run_generation)\n",
        "\n",
        "# Output Area\n",
        "out_area = widgets.Output()\n",
        "\n",
        "# ==========================================\n",
        "# UI LAYOUT\n",
        "# ==========================================\n",
        "\n",
        "ui_header = widgets.HTML(\"<h2>Z-Image-Turbo Colab UI (T4 Optimized)</h2>\")\n",
        "\n",
        "# Left Column (Prompts)\n",
        "left_col = widgets.VBox([\n",
        "    ui_header,\n",
        "    positive_prompt,\n",
        "    negative_prompt,\n",
        "    gen_btn,\n",
        "    out_area\n",
        "])\n",
        "\n",
        "# Right Column (Settings)\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Image Settings</b>\"),\n",
        "    aspect_ratio,\n",
        "    widgets.HBox([width, height]),\n",
        "    widgets.HTML(\"<br><b>Sampling Settings</b>\"),\n",
        "    steps,\n",
        "    cfg,\n",
        "    seed,\n",
        "    sampler_name,\n",
        "    scheduler,\n",
        "    denoise\n",
        "])\n",
        "\n",
        "# Main Layout\n",
        "ui = widgets.HBox([left_col, right_col])\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "dYfZV892PPWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
