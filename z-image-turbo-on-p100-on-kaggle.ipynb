{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- PART 1: SETUP (UPDATED FOR YOUR REPOS) ---\n\n# Define the working directory for Kaggle\nWORK_DIR = \"/kaggle/working/ComfyUI\"\n\nimport os\n\n# 1. Clone from YOUR personal GitHub fork\nif not os.path.exists(WORK_DIR):\n    # UPDATED: Using your fork URL\n    !git clone https://github.com/Imtiazul-Islam/ComfyUI {WORK_DIR}\n\n# Navigate into the directory\n%cd {WORK_DIR}\n\n# Install requirements (quietly)\n!pip install -q -r requirements.txt\n\n# Install aria2 for fast downloads\n!apt-get -y install -qq aria2\n\n# 2. Download the models using aria2 from YOUR Hugging Face Mirror\n# We point to your 'ImtiazulIslam' account now.\n# We still use the -d flag to put them in the correct ComfyUI subfolders.\n\n# Model 1: UNET (Diffusion Model)\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d {WORK_DIR}/models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n\n# Model 2: CLIP (Text Encoder)\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/qwen_3_4b.safetensors -d {WORK_DIR}/models/clip -o qwen_3_4b.safetensors\n\n# Model 3: VAE\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ImtiazulIslam/Z-Image-Turbo-FP8/resolve/main/ae.safetensors -d {WORK_DIR}/models/vae -o ae.safetensors\n\nprint(\"Setup Complete. Models loaded from your personal repositories.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- PART 2: LOAD MODELS & UI (CORRECTED) ---\n\n%cd {WORK_DIR}\n\nimport os, random, time, datetime\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom IPython.display import display, clear_output\nimport ipywidgets as widgets\n\n# Import ComfyUI Nodes\nfrom nodes import NODE_CLASS_MAPPINGS\n\n# Load Nodes\nUNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\nCLIPLoader = NODE_CLASS_MAPPINGS[\"CLIPLoader\"]()\nVAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\nCLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\nKSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\nVAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\nEmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n\nprint(\"Loading Models (this may take a moment)...\")\n\n# Load Models into VRAM\nwith torch.inference_mode():\n    unet = UNETLoader.load_unet(\"z-image-turbo-fp8-e4m3fn.safetensors\", \"fp8_e4m3fn_fast\")[0]\n    clip = CLIPLoader.load_clip(\"qwen_3_4b.safetensors\", type=\"lumina2\")[0]\n    vae = VAELoader.load_vae(\"ae.safetensors\")[0]\n\nprint(\"Models Loaded. Initializing UI...\")\n\n# ==========================================\n# UI GENERATION LOGIC\n# ==========================================\n\n@torch.inference_mode()\ndef run_generation(b):\n    # Disable button while generating\n    gen_btn.disabled = True\n    gen_btn.description = \"Generating...\"\n    out_area.clear_output(wait=True)\n    \n    with out_area:\n        print(f\"Starting generation: {width.value}x{height.value} | Steps: {steps.value}\")\n        \n        tmp_dir = f\"{WORK_DIR}/output\"\n        os.makedirs(tmp_dir, exist_ok=True)\n        \n        # 1. Handle Seed\n        current_seed = seed.value\n        if current_seed == 0:\n            random.seed(int(time.time()))\n            current_seed = random.randint(0, 18446744073709551615)\n            \n        # 2. Encode Prompts\n        positive = CLIPTextEncode.encode(clip, positive_prompt.value)[0]\n        negative = CLIPTextEncode.encode(clip, negative_prompt.value)[0]\n        \n        # 3. Generate Latent\n        latent_image = EmptyLatentImage.generate(width.value, height.value, batch_size=1)[0]\n        \n        # 4. Sample\n        samples = KSampler.sample(\n            unet, \n            current_seed, \n            steps.value, \n            cfg.value, \n            sampler_name.value, \n            scheduler.value, \n            positive, \n            negative, \n            latent_image, \n            denoise=denoise.value\n        )[0]\n        \n        # 5. Decode\n        decoded = VAEDecode.decode(vae, samples)[0].detach()\n        \n        # 6. Save with Timestamp\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"z_turbo_{timestamp}.png\"\n        output_path = f\"{tmp_dir}/{filename}\"\n        Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(output_path)\n        \n        # 7. Display\n        print(f\"Done! Seed: {current_seed}\")\n        print(f\"Saved to: {output_path}\")\n        display(Image.open(output_path))\n        \n    # Re-enable button\n    gen_btn.disabled = False\n    gen_btn.description = \"Generate Image\"\n\ndef update_dimensions(change):\n    \"\"\"Auto-update Width/Height based on Aspect Ratio selection\"\"\"\n    ar = change['new']\n    \n    if ar == \"1:1 (Square)\":\n        width.value, height.value = 1024, 1024\n    elif ar == \"16:9 (Landscape)\":\n        width.value, height.value = 1024, 576\n    elif ar == \"9:16 (Portrait)\":\n        width.value, height.value = 576, 1024\n    elif ar == \"4:3 (Photo)\":\n        width.value, height.value = 1024, 768\n    elif ar == \"3:2 (Classic)\":\n        width.value, height.value = 960, 640\n    elif ar == \"21:9 (Ultrawide)\":\n        width.value, height.value = 1344, 576\n\n# ==========================================\n# WIDGET DEFINITIONS (FIXED NAMES)\n# ==========================================\n\nstyle = {'description_width': 'initial'}\n\n# Prompts\npositive_prompt = widgets.Textarea(\n    value=\"a cinematic photo of a cat, realistic, 8k\",\n    description='Positive Prompt:', style=style, layout=widgets.Layout(width='95%')\n)\nnegative_prompt = widgets.Textarea(\n    value=\"blurry, ugly, bad quality, distorted\",\n    description='Negative Prompt:', style=style, layout=widgets.Layout(width='95%')\n)\n\n# Aspect Ratio & Dimensions\naspect_ratio = widgets.Dropdown(\n    options=[\"1:1 (Square)\", \"16:9 (Landscape)\", \"9:16 (Portrait)\", \"4:3 (Photo)\", \"3:2 (Classic)\", \"21:9 (Ultrawide)\", \"Custom\"],\n    value=\"1:1 (Square)\",\n    description='Aspect Ratio:', style=style\n)\naspect_ratio.observe(update_dimensions, names='value')\n\n# FIXED: Renamed w->width and h->height to match the usage in run_generation\nwidth = widgets.IntText(value=1024, description='Width:', style=style)\nheight = widgets.IntText(value=1024, description='Height:', style=style)\n\n# Settings\nsteps = widgets.IntSlider(value=9, min=1, max=20, step=1, description='Steps:', style=style)\ncfg = widgets.FloatSlider(value=1.0, min=1.0, max=5.0, step=0.1, description='CFG Scale:', style=style)\nseed = widgets.IntText(value=0, description='Seed (0=Random):', style=style)\nsampler_name = widgets.Dropdown(\n    options=['euler', 'euler_ancestral', 'dpmpp_2m', 'dpmpp_sde'],\n    value='euler', description='Sampler:', style=style\n)\nscheduler = widgets.Dropdown(\n    options=['simple', 'normal', 'karras', 'exponential', 'sgm_uniform'],\n    value='simple', description='Scheduler:', style=style\n)\ndenoise = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.05, description='Denoise:', style=style)\n\n# Generate Button\ngen_btn = widgets.Button(\n    description='Generate Image',\n    button_style='success', \n    tooltip='Click to generate',\n    icon='magic'\n)\ngen_btn.on_click(run_generation)\n\n# Output Area\nout_area = widgets.Output()\n\n# ==========================================\n# UI LAYOUT\n# ==========================================\n\nui_header = widgets.HTML(\"<h3>Z-Image-Turbo Native UI</h3>\")\n\n# Left Column (Prompts & Generation)\nleft_col = widgets.VBox([\n    ui_header,\n    positive_prompt,\n    negative_prompt,\n    gen_btn\n])\n\n# Right Column (Settings)\nright_col = widgets.VBox([\n    aspect_ratio,\n    widgets.HBox([width, height]), # FIXED: Used width and height here\n    steps,\n    cfg,\n    seed,\n    sampler_name,\n    scheduler,\n    denoise\n])\n\n# Main Layout\nui = widgets.HBox([left_col, right_col])\n\n# Display\ndisplay(ui)\ndisplay(out_area)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
